{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 6: Working with Geocoded Data\n",
    "\n",
    "### Building Maps in geopandas\n",
    "In this lesson we will download COVID-19 data from data.world. We will normalize the data to compare spread between counties. Were we to simply plot the total number of cases or deaths by county, the results would be biased as counties with larger populations would likely have more cases and more deaths. We will observe how the spread developed across the country, starting in the northeast, eventually making its way to other regions.\n",
    "\n",
    "**Installing geopandas**\n",
    "\n",
    "Although there is a geopandas installation available using the conda install command in you command line shell, that package is incomplete for our purposes. We will need to install dependencies - in this order: GDAL,Fiona, and Shapely - for geopandas before installing geopandas. I have included the .whl files for each of these packages in the same folder is this notebook. Download the files and save them to your local folder. To install, use the command:\n",
    "\n",
    "> pip install filename\n",
    "\n",
    "If you are using a machine for which you are not the administrator, use the command:\n",
    "\n",
    "> pip install filename --user\n",
    "\n",
    "Install the libraries using these commands from an administrator shell or using --user at the end of the statement:\n",
    "\n",
    "> pip install GDAL-3.1.4-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "> pip install Fiona-1.8.17-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "> pip install Shapely-1.7.1-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "If you are using Python 3.8, use the following wheels:\n",
    "\n",
    "> pip install GDAL-3.1.4-cp38-cp38-win_amd64.whl\n",
    "\n",
    "> pip install Fiona-1.8.17-cp38-cp38-win_amd64.whl\n",
    "\n",
    "> pip install Shapely-1.7.1-cp38-cp38-win_amd64.whl\n",
    "\n",
    "If you are using a newer version of python and it's giving you issues installing GDAL, Fiona and Shapely, use the command line or powershell to downgrade/upgrade the version to 3.7:\n",
    "> conda install python =3.7 anaconda=custom\n",
    "\n",
    "Once you've successfully changed the version to 3.7, you can then run the pip commands above to install GDAL, Fiona and Shapely. \n",
    "\n",
    "If you are using a mac, you may install the appropriate module by selecting the version.\n",
    "\n",
    "> pip install -v GDAL==3.1.4\n",
    "\n",
    "> pip install -v Fiona==1.8.17\n",
    "\n",
    "> pip install -v Shapely==1.7.1\n",
    "\n",
    "Finally, install geopandas:\n",
    "\n",
    "> pip install geopandas\n",
    "\n",
    "### Downloading the COVID-19 data\n",
    "\n",
    "We will use two datasets. First, we will import a shapefile to use with geopandas, which we will later use to generate a county level map that tracks COVID-19. The shapefile is provide for you in the Github folder housing this post. You can also download shapefiles from the [U.S. Census website](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html). We will download Johns Hopkins's COVID-19 data from the Associated Press's [account](https://data.world/associatedpress/johns-hopkins-coronavirus-case-tracker) at data.world using their [Python module](https://data.world/integrations/python). Follow [these instructions](https://github.com/datadotworld/data.world-py/) to install the datadotworld module and access their API.\n",
    "\n",
    "> Datadotworld may be useful efficiently collecting data for class projects, so keep this libary in mind as you make plans for your project.\n",
    "\n",
    "**Note:**\n",
    "> Checkout this **[article](https://www.ndsu.edu/centers/pcpe/news/detail/58432/)** from NDSU PCPE website for some additional information on COVID-19 data analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COVID19Map.py\n",
    "### import all modules that we will use in this lesson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "\n",
    "# datadotworld will connect to the John's hopkin's python API\n",
    "import datadotworld as dw\n",
    "\n",
    "# We won't actually use datetime directly. Since the dataframe index will use \n",
    "# data formatted as datetime64, I import it in case I need to use the datetime module to troubleshoot later \n",
    "import datetime\n",
    "\n",
    "# you could technically call many of the submodules from matplotlib using mpl., \n",
    "#but for convenience we explicitly import submodules. These will be used for constructing visualizations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# If you choose to make a dynamic visualization for the homework\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.ticker as mtick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to download our covid data from John Hopkins website\n",
    "def import_covid_data(filename, FIPS_name):\n",
    "    # Load COVID19 county data using datadotworld API\n",
    "    # Data provided by Johns Hopkins, file provided by Associated Press\n",
    "    dataset = dw.load_dataset(\n",
    "        \"associatedpress/johns-hopkins-coronavirus-case-tracker\",\n",
    "        auto_update = True)\n",
    "    \n",
    "    # the dataset includes multiple dataframes. We will only use #2\n",
    "    covid_data = dataset.dataframes[\"2_cases_and_deaths_by_county_timeseries\"]\n",
    "    \n",
    "    # Include only oberservation for political entities within states\n",
    "    # i.e., not territories, etc... drop any nan fip values with covid_data[FIPS_name] > 0\n",
    "    # so, look at covid data df named covid_data, \n",
    "    # filter observations by the FIPS_name column where it's less than 57000 or greater than 0\n",
    "    # the first 2 digits of the fip_code identify the state (google county fips codes)\n",
    "    # the following 3 digits identify the county\n",
    "    # Note that fips_code > 57000 gives you territories and not actual states (google fips code)\n",
    "    covid_data = covid_data[covid_data[FIPS_name] < 57000]\n",
    "    covid_data = covid_data[covid_data[FIPS_name] > 0]\n",
    "\n",
    "    # Transform FIPS codes into integers (not floats)\n",
    "    covid_data[FIPS_name] = covid_data[FIPS_name].astype(int)\n",
    "    \n",
    "    # set the index as fips and date. The index will make it easy pandas to access/inteprete the data and df\n",
    "    # inplace = true says make the change in the same df. No need to create a new df with the change\n",
    "    covid_data.set_index([FIPS_name, \"date\"], inplace = True)\n",
    "    \n",
    "    # Prepare a column for state abbreviations. We will draw these from a\n",
    "    # state_dict dictionary created in the next step.\n",
    "    \n",
    "    # first, let's initialize the column by creating a blank string (\"\") since the column will only have str. values\n",
    "    covid_data[\"state_abr\"] = \"\"\n",
    "    \n",
    "    # for each entry, we'll use the abbreviations in the state_dict. \n",
    "    # where the state name = state_x, use the abbreviation from State_dict that corresponds to that state.\n",
    "    for state, abr in state_dict.items():\n",
    "        \n",
    "        # .loc[row(s), col] says call covid data, for all rows where the state == whatever state we are on, \n",
    "        # enter a value under state abbr. for all those rows\n",
    "        covid_data.loc[covid_data[\"state\"] == state, \"state_abr\"] = abr\n",
    "        \n",
    "    # Create \"Location\" which concatenates county name and state abbreviation \n",
    "    # save location name as County, Abr (e.g. Cass, ND)\n",
    "    covid_data[\"Location\"] = covid_data[\"location_name\"] + \", \" + covid_data[\"state_abr\"]\n",
    "\n",
    "    return covid_data\n",
    "\n",
    "\n",
    "# create Geo dataframe\n",
    "def import_geo_data(filename, index_col = \"Date\", FIPS_name = \"FIPS\"):\n",
    "    \n",
    "    # use geopandas to import county level shapefile\n",
    "    map_data = geopandas.read_file(filename = filename, index_col = index_col)\n",
    "    \n",
    "    # rename fips code to match variable name in COVID-19 data\n",
    "    map_data.rename(columns={\"State\":\"state\"}, inplace = True)\n",
    "    \n",
    "    # Combine statefips and county fips to create a single fips value\n",
    "    # that identifies each particular county without referencing the state separately\n",
    "    map_data[FIPS_name] = map_data[\"STATEFP\"].astype(str) + map_data[\"COUNTYFP\"].astype(str)\n",
    "    map_data[FIPS_name] = map_data[FIPS_name].astype(np.int64)\n",
    "    \n",
    "    # set FIPS as index\n",
    "    map_data.set_index(FIPS_name, inplace=True)\n",
    "    \n",
    "    return map_data\n",
    "\n",
    "\n",
    "# I include this dictionary to convenienlty cross reference state names and state abbreviations.\n",
    "state_dict = {'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', \n",
    "              'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT','Delaware': 'DE', \n",
    "              'District of Columbia': 'DC', 'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', \n",
    "              'Idaho': 'ID', 'Illinois': 'IL','Indiana': 'IN', 'Iowa': 'IA','Kansas': 'KS', \n",
    "              'Kentucky': 'KY','Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA',\n",
    "              'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', \n",
    "              'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', \n",
    "              'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "              'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', \n",
    "              'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', \n",
    "              'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_name = \"fips_code\"\n",
    "covid_filename = \"COVID19DataAP.csv\"\n",
    "\n",
    "# rename_FIPS matches map_data FIPS with COVID19 FIPS name\n",
    "covid_data = import_covid_data(filename = covid_filename, FIPS_name = fips_name)\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that counties are identified by a state, with reference to the states fip code, and by a county level fip code. Together, these make the total fip code. We will use the fip code to align the data frames.\n",
    "\n",
    "Notice also that each observation is associated with an entry under **geometry.** Each entry has identified with it the shape and location of the county identified by a series of coordinates representing the perimeter of the county.\n",
    "\n",
    "Next, let's call the COVID-19 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data = import_geo_data(filename = \"countiesWithStatesAndPopulation.shp\",\n",
    "                           index_col = \"Date\", FIPS_name= fips_name)\n",
    "\n",
    "#covid_data = import_covid_data(filename = covid_filename, FIPS_name = fips_name)\n",
    "map_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print a particular record in the df. 38017 is the fips code index and we want to return \n",
    "# the record in the geometry column that also corresponds to fips_code 21007. Since the geometry column\n",
    "# is made of coordinates, it plots a polygon using the coordinates which is actually the shape of the county\n",
    "\n",
    "map_data.loc[38017, \"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you prefer seeing the text value rather than the shape of the coordinate, you can use the \"print\" command\n",
    "print(map_data.loc[38017, \"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot all the data at once using df.plot(). However, we'll want to specify map parameters\n",
    "# Note: you might need to \"pip install descartes\" to plot the entire map (or all the polygons what makes up the map)\n",
    "map_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COVID-19 data also includes a fip_code column in the index. Notice that the index also includes the date. To transform the covid_data into a geodataframe, we will need to duplicate the geodataframe for every date in the multi-index of covid_data. Once the geodataframe has been developed with an index that matches the covid_data, we can add every column from covid_data to the geodataframe by using a for loop to cycle three each key and column of data (key, val) in covid_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COVID19Map.py\n",
    "# . . .\n",
    "# create pandas geo df\n",
    "def create_covid_geo_dataframe(covid_data, map_data, dates):\n",
    "    # create geopandas dataframe with multiindex for date\n",
    "    # original geopandas dataframe had no dates, so copies of the df are \n",
    "    # stacked vertically, with a new copy for each date in the covid_data index\n",
    "    #(dates is a global)\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        \n",
    "        # select county observations from each date in dates\n",
    "        df = covid_data[covid_data.index.get_level_values(\"date\")==date]\n",
    "        \n",
    "        # use the fips_codes from the slice of covid_data to select counties\n",
    "        # from the map_data index,making sure that the map_data index matches\n",
    "        # the covid_data index\n",
    "        counties = df.index.get_level_values(\"fips_code\")\n",
    "        \n",
    "        # call a slice of the geodataframe that includes the counties that are in covid_data\n",
    "        agg_df = map_data.loc[counties]\n",
    "       \n",
    "        # each row should reflect the date so that it is aligned with covid_data\n",
    "        agg_df[\"date\"] = date\n",
    "        if i == 0:\n",
    "            \n",
    "            # create the geodataframe, select coordinate system (.crs) to match map_data.crs\n",
    "            matching_gpd = geopandas.GeoDataFrame(agg_df, crs = map_data.crs)\n",
    "            i += 1\n",
    "        else:\n",
    "            \n",
    "            # after initial geodataframe is created, stack a dataframe for each date in dates. \n",
    "            # Once completed, index of matching_gpd will match index of covid_data\n",
    "            matching_gpd = matching_gpd.append(agg_df, ignore_index = False) \n",
    "            \n",
    "    # Set mathcing_gpd index as[\"fips_code\", \"date\"], liked covid_data index\n",
    "    matching_gpd.reset_index(inplace=True)\n",
    "    matching_gpd.set_index([\"fips_code\",\"date\"], inplace = True)\n",
    "    \n",
    "    # add each column from covid_data to mathcing_gpd\n",
    "    for key, val in covid_data.items():\n",
    "        matching_gpd[key] = val\n",
    "    return matching_gpd       \n",
    "\n",
    "# . . . to end of script . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of dates\n",
    "# dates will be used to create a geopandas DataFrame with multiindex \n",
    "# set(lst) removes duplicate entries\n",
    "dates = sorted(list(set(covid_data.index.get_level_values(\"date\"))))\n",
    "covid_data = create_covid_geo_dataframe(covid_data, map_data, dates)\n",
    "\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will normalize the data by population using the create_new_vars() function. We will use the population data that I have included with the .shp file under the key \"Population\". If you have downloaded the shapefile directly from the census website, than you might prefer to use \"total_population\", which is included in the Associated Press's COVID-19 data.\n",
    "\n",
    "Calculate deaths as a percent of population, then multiply that percentage by $10^6$ (one million) to calculate deaths per million. In addition to normalizing the data by population, we will also calculate the 7 day running average of this variable using the .rolling() method followed by .mean()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COVID19Map.py\n",
    "# . . .\n",
    "def create_new_vars(covid_data, moving_average_days):\n",
    "    \n",
    "    # use a for loop that performs the same operations on data for cases and for deaths\n",
    "    for key in [\"cases\", \"deaths\"]:\n",
    "        \n",
    "        # create a version of the key with the first letter of each word capitalized\n",
    "        cap_key = key.title()\n",
    "        covid_data[cap_key + \" per Million\"] = covid_data[\"cumulative_\" + key]\\\n",
    "            .div(covid_data[\"total_population\"]).mul(10 ** 6)\n",
    "        \n",
    "        # generate daily data normalized per million population by taking the daily difference within each\n",
    "        # entity (covid_data.index.names[0]), dividing this value by population and multiplying that value\n",
    "        # by 1 million 10 ** 6\n",
    "        # can also use .groupby(\"figs_code\")\n",
    "        covid_data[\"Daily \" + cap_key + \" per Million\"] =\\\n",
    "            covid_data[\"cumulative_\" + key].groupby(covid_data.index.names[0]).diff(1)\\\n",
    "            .div(covid_data[\"total_population\"]).mul(10 ** 6)\n",
    "        \n",
    "        # taking the rolling average; choice of number of days is passed as moving_average_days\n",
    "        # .rolling lets python hold the variables but not record it then take the mean of the values\n",
    "        covid_data[\"Daily \" + cap_key + \" per Million MA\"] = covid_data[\"Daily \" + cap_key + \" per Million\"]\\\n",
    "                   .rolling(moving_average_days).mean()\n",
    "# . . .\n",
    "moving_average_days = 7\n",
    "create_new_vars(covid_data, moving_average_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to create our first map! Much of the same structure for plotting other visualizations in matplotlib using pandas also applies to using geopandas to create maps. We will initiate a figure using plt.subplots(). Since we are plotting time series data, we will need to plot only one date at a time. Let's start with the most recent data available.\n",
    "\n",
    "To start, we need to select data from counties within the 48 states, otherwise Alaska and Hawaii will be include. This allows the area represented to be much smaller. The measures that are slected are rough estimates, but all that matters is that the 48 states reside within the boundaries chosen. Similar to the data_processed variable, I include a map_bounded variable so that this computation will not be repeated when the script is run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COVID19Map.py\n",
    "#. . . \n",
    "\n",
    "# only include observations within these boundaries (minx, maxx, miny, maxy)\n",
    "# this will shrink the size of the map\n",
    "# maxx and maxy refer to max x and y coordinates respectively\n",
    "# minx and miny refer to min x and y coordinates\n",
    "def select_data_within_bounds(data, minx, miny, maxx, maxy):\n",
    "    data = data[data.bounds[\"maxx\"] <= maxx]\n",
    "    data = data[data.bounds[\"maxy\"] <= maxy]\n",
    "    data = data[data.bounds[\"minx\"] >= minx]\n",
    "    data = data[data.bounds[\"miny\"] >= miny]\n",
    "    \n",
    "    return data\n",
    "# . . . to end of script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose most recent date in data\n",
    "date = dates[-1]\n",
    "\n",
    "# choose map bounds\n",
    "if \"map_bounded\" not in locals():\n",
    "    # define boundary coordinates\n",
    "    minx = -127\n",
    "    miny = 23\n",
    "    maxx = -66\n",
    "    maxy = 48\n",
    "    \n",
    "    # call function and redefine data\n",
    "    covid_data = select_data_within_bounds(covid_data, minx, miny, maxx, maxy)\n",
    "    map_bounded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot map\n",
    "fig, ax = plt.subplots(figsize=(18,8),\n",
    "                       subplot_kw = {'aspect': 'equal'})   \n",
    "\n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "key = \"Deaths per Million\"\n",
    "\n",
    "# define df for subset of data to only include datas we're concerned about\n",
    "df = covid_data[covid_data.index.get_level_values(\"date\") == date]\n",
    "df.plot(ax=ax, cax = ax, column=key, linewidth = 0.5, edgecolor='lightgrey')\n",
    "\n",
    "ax.set_title(str(date) + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks okay. A few things can be improved. First, it's usually a good idea to select a colormap that spans a narrow range of colors. For this purpose, we will choose to use the colorbar defined by \"Reds\" in matplotlib. Color choice also conveys affect to the reader. A choice of color, say \"Blues\", might strike the viewer as conveying neutral content. Our choice of red conveys that areas that are darker shades of red tend to have been more greatly impacted by the COVID-19. The variable \"Deaths per Million\" normalizes death by population, allowing for a fair comparison between counties.\n",
    "\n",
    "We will also want to limit the variety of shades used for representation. It is easier to interpret data that has been divided into discrete categories. We will choose to use 4 categories representing ranges of increasing size and breadth.\n",
    "\n",
    "Since we are plotting levels of values, it will be useful to compare logged values. This will give us a sense that some areas are doing better or worse by orders of magnitude, depending on the plot, about 10 to 20 times better or worse given each change in color. To have colors represent a range of logged values, we use the cm.colors.LogNorm() and apply this with plt.cm.ScalarMappable(). We explicitly identify min and max values as this will be useful to us when we plot animations over time later in this lesson.\n",
    "\n",
    "Finally, in the title, we identify the date, region, and the category of data represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#COVID19Map.py\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8),\n",
    "        subplot_kw = {'aspect': 'equal'}) \n",
    "\n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "key = \"Deaths per Million\"\n",
    "\n",
    "# change colors, divide into 4 distinct colors\n",
    "# if you google matplotlib color maps, you'll have names of color themes to use. \n",
    "# That's how we got \"Reds\" color theme for the map. \n",
    "# We could replace \"Reds\" with Purples, Greys, OrRd or any other color theme found here \n",
    "# https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "cmap = cm.get_cmap('Reds', 4)\n",
    "\n",
    "# let's set our vmin log value to 1 and vmax value to the highest value in our df.\n",
    "\n",
    "vmin = 1 \n",
    "vmax = df[key].max()\n",
    "\n",
    "# we'll use the vmin and vmax to create the color bar legend as seen two maps below\n",
    "# norm defines the range of the logged values\n",
    "norm = cm.colors.LogNorm(vmin=vmin, vmax =vmax)\n",
    "\n",
    "# scalarMappable uses that range in the colorbar\n",
    "plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "df = covid_data[covid_data.index.get_level_values(\"date\") == date]\n",
    "df.plot(ax=ax, cax = ax, column=key, vmin=vmin ,vmax = vmax, \n",
    "        cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "        norm = norm)\n",
    "\n",
    "ax.set_title(str(date) + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to include a colorbar so that the viewer knows what values are conveyed by map colors. This requires the addition of several parameters that allow for inclusion of a colorbar legend. To construct the colorbar, we need to call make_axes_locatable(ax), then use this to create a colorbar axis, cax. Using cax and cmap, created in the block of code executed in the previous section, we are able to create a colorbar. To control the value format, we create a list of the axis values, making sure that they are integers (not floats). Then, identify the newly created matplotlib objects in the .plot() method.\n",
    "\n",
    "Since the color axis is logged, technically it doesn't include a representation for 0. When passing the dataframe, replace all 0 values with 1 in order to transform the white counties into beige. If you were to use this map, be sure to note that you have made this adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,8),\n",
    "        subplot_kw = {'aspect': 'equal'})   \n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "key = \"Deaths per Million\"\n",
    "df = covid_data[covid_data.index.get_level_values(\"date\")==date]\n",
    "cmap = cm.get_cmap('Reds', 4)\n",
    "vmin = 1 \n",
    "vmax = df[key].max()\n",
    "norm = cm.colors.LogNorm(vmin=vmin, vmax =vmax)\n",
    "\n",
    "############## Add Colorbar ################\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "\n",
    "# prepare space for colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "size = \"5%\" \n",
    "cax = divider.append_axes(\"right\", size = size, pad = 0.1)\n",
    "\n",
    "# add colorbar to figure\n",
    "cbar = fig.colorbar(sm, cax=cax, cmap = cmap)\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "vals = list(cbar.ax.get_yticks())\n",
    "vals.append(vmax)\n",
    "\n",
    "# format colorbar values as int\n",
    "cbar.ax.set_yticklabels([int(x) for x in vals])\n",
    "cbar.ax.set_ylabel(key, fontsize = 20)\n",
    "\n",
    "df.plot(ax=ax, cax = ax, column=key, vmin=vmin ,vmax = vmax, \n",
    "             cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "             norm = norm)\n",
    "ax.set_title(str(date)[:10] + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully made a quality map. Next, let's use a for loop to create maps for all for categories that we plotted in the previous post. This is simple. First we define a list of keys, and iterate over each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = [\"Cases per Million\", \"Deaths per Million\", \n",
    "        \"Daily Cases per Million MA\", \"Daily Deaths per Million MA\"]\n",
    "for key in keys:\n",
    "    # identify whether or not to log values for color axis\n",
    "    # if daily rates, do not log. Only log totals.\n",
    "    log = False if \"Daily\" in key else True\n",
    "    fig, ax = plt.subplots(figsize=(18,8),\n",
    "        subplot_kw = {'aspect': 'equal'})   \n",
    "    plt.rcParams.update({\"font.size\": 30})\n",
    "    plt.xticks(fontsize = 25)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    \n",
    "    # this time we replace 0 values with 1\n",
    "    # so that these values show up as beige instead of as white\n",
    "    # when color axis is logged\n",
    "    df = covid_data[covid_data.index.get_level_values(\"date\")==date].replace(0,1)\n",
    "    \n",
    "    # set range of colorbar\n",
    "    vmin = 1 if log else 0 \n",
    "    vmax = df[key].max()\n",
    "    \n",
    "    # choose colormap\n",
    "    cmap = cm.get_cmap('Reds', 4)\n",
    "    \n",
    "    # format colormap\n",
    "    if log:\n",
    "        norm = cm.colors.LogNorm(vmin=vmin, vmax =vmax)\n",
    "    else:\n",
    "        norm = cm.colors.Normalize(vmin = vmin, vmax = vmax)\n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    \n",
    "    # empty array for the data range\n",
    "    sm._A = []\n",
    "    \n",
    "    # prepare space for colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    size = \"5%\" \n",
    "    cax = divider.append_axes(\"right\", size = size, pad = 0.1)\n",
    "    \n",
    "    # add colorbar to figure\n",
    "    cbar = fig.colorbar(sm, cax=cax, cmap = cmap)\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    vals = list(cbar.ax.get_yticks())\n",
    "    vals.append(vmax)\n",
    "\n",
    "    # format colorbar values as int\n",
    "    cbar.ax.set_yticklabels([int(x) for x in vals])\n",
    "    cbar.ax.set_ylabel(key, fontsize = 20)\n",
    "\n",
    "\n",
    "    df.plot(ax=ax, cax = cax, column=key, vmin=vmin ,vmax = vmax, \n",
    "                 cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "                 norm = norm)\n",
    "    ax.set_title(str(date)[:10] + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initiate/create empty pdf file\n",
    "pp = PdfPages(\"COVID-19Maps.pdf\")\n",
    "\n",
    "keys = [\"Cases per Million\", \"Deaths per Million\", \n",
    "        \"Daily Cases per Million MA\", \"Daily Deaths per Million MA\"]\n",
    "\n",
    "# take dates as a set, then make a list out of it, then sort it. \n",
    "dates = sorted(\n",
    "    list(\n",
    "        set(covid_data.index.get_level_values(\"date\"))))\n",
    "\n",
    "# only include every 7th day. It's obviosly not required but\n",
    "# it should speed up how fast the program runs the code  \n",
    "dates = dates[::7]\n",
    "for key in keys:\n",
    "    # set range of colorbar\n",
    "    vmin = 1   \n",
    "    vmax = df[key].max()\n",
    "        \n",
    "    for date in dates:\n",
    "        # identify whether or not to log values for color axis\n",
    "        # if daily rates, do not log. Only log totals.\n",
    "        log = False if \"Daily\" in key else True\n",
    "        fig, ax = plt.subplots(figsize=(18,8), \n",
    "                               subplot_kw = {'aspect': 'equal'})  \n",
    "        \n",
    "        plt.rcParams.update({\"font.size\": 30})\n",
    "        plt.xticks(fontsize = 25)\n",
    "        plt.yticks(fontsize = 25)\n",
    "\n",
    "        # this time we replace 0 values with 1\n",
    "        # so that these values show up as beige instead of as white\n",
    "        # when color axis is logged\n",
    "        df = covid_data[covid_data.index.get_level_values(\"date\")==date]\n",
    "\n",
    "        # choose colormap\n",
    "        cmap = cm.get_cmap('Reds', 4)\n",
    "\n",
    "        # format colormap\n",
    "        if log:\n",
    "            norm = cm.colors.LogNorm(vmin=vmin, vmax =vmax)\n",
    "        else:\n",
    "            norm = cm.colors.Normalize(vmin = vmin, vmax = vmax)\n",
    "        sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "        # empty array for the data range\n",
    "        sm._A = []\n",
    "\n",
    "        # prepare space for colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        size = \"5%\" \n",
    "        cax = divider.append_axes(\"right\", size = size, pad = 0.1)\n",
    "\n",
    "        # add colorbar to figure\n",
    "        cbar = fig.colorbar(sm, cax=cax, cmap = cmap)\n",
    "        cbar.ax.tick_params(labelsize=18)\n",
    "        vals = list(cbar.ax.get_yticks())\n",
    "        vals.append(vmax)\n",
    "\n",
    "        # format colorbar values as int\n",
    "        cbar.ax.set_yticklabels([int(x) for x in vals])\n",
    "        cbar.ax.set_ylabel(key, fontsize = 20)\n",
    "\n",
    "\n",
    "        df.plot(ax=ax, cax = cax, column=key, vmin=vmin ,vmax = vmax, \n",
    "                     cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "                     norm = norm)\n",
    "        ax.set_title(str(date)[:10] + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        # save in pdf file\n",
    "        pp.savefig(fig, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        \n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select data that falls within bounds of coordinates\n",
    "def select_data_within_bounds(data, minx, miny, maxx, maxy):\n",
    "    data = data[data.bounds[\"maxx\"] <= maxx]\n",
    "    data = data[data.bounds[\"maxy\"] <= maxy]\n",
    "    data = data[data.bounds[\"minx\"] >= minx]\n",
    "    data = data[data.bounds[\"miny\"] >= miny]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def plot_map(*kwargs):\n",
    "    plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    plot_df = df[df.index.get_level_values(\"date\")==date]\n",
    "    plot_df.plot(ax=ax, cax = ax, column=key, vmin=vmin ,vmax = vmax, \n",
    "                 cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "                 norm = norm)\n",
    "    ax.set_title(str(date)[:10] + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)\n",
    "    \n",
    "def init(*kwargs):\n",
    "    size = \"5%\"     \n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    # empty array for the data range\n",
    "    sm._A = []\n",
    "    # add the colorbar to the figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size = size, pad = 0.1)\n",
    "    # add colorbar to figure\n",
    "    cbar = fig.colorbar(sm, cax=cax, cmap = cmap)\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    vals = list(cbar.ax.get_yticks())\n",
    "    vals.append(vmax)\n",
    "    if \"Daily\" not in key: vals[0] = 0 \n",
    "    # format colorbar with logged or observed values\n",
    "    if log:\n",
    "        cbar.ax.yaxis.set_major_formatter(mtick.LogFormatter())\n",
    "    else:\n",
    "        cbar.ax.yaxis.set_major_formatter(mtick.Formatter())\n",
    "    # format colorbar values as int\n",
    "    cbar.ax.set_yticklabels([int(x) for x in vals])\n",
    "    cbar.ax.set_ylabel(key, fontsize = 20)\n",
    "    \n",
    "date = dates[-1]\n",
    "keys = [\"Cases per Million\", \"Deaths per Million\", \n",
    "        \"Daily Cases per Million MA\", \"Daily Deaths per Million MA\"]\n",
    "\n",
    "if \"map_bounded\" not in locals():\n",
    "    minx = -127\n",
    "    miny = 23\n",
    "    maxx = -58\n",
    "    maxy = 54\n",
    "    covid_data = select_data_within_bounds(covid_data, minx, miny, maxx, maxy)\n",
    "    map_bounded = True\n",
    "\n",
    "\n",
    "for key in keys:\n",
    "    log = False if \"Daily\" in key else True\n",
    "    # this time we replace 0 values with 1\n",
    "    # so that these values show up as beige  instead of as white\n",
    "    # when color axis is logged\n",
    "    vmin = 1 if log else 0 \n",
    "    vmax = df[key][df.index.get_level_values(\"date\") == date].max()\n",
    "    # Create colorbar as a legend\n",
    "    cmap = cm.get_cmap('Reds', 4)\n",
    "    if log:\n",
    "        norm = cm.colors.LogNorm(vmin=vmin, vmax =vmax)\n",
    "    else:\n",
    "        norm = cm.colors.Normalize(vmin = vmin, vmax = vmax)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18,8),\n",
    "        subplot_kw = {'aspect': 'equal'})   \n",
    "    plt.rcParams.update({\"font.size\": 30})\n",
    "    plt.xticks(fontsize = 25)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    # the functions will unpack the tuple. The same names variable names\n",
    "    # are used in the function\n",
    "    kwargs = (df, key, log, date, fig, ax, cmap, norm, vmin, vmax)\n",
    "    init(kwargs)\n",
    "    plot_map(kwargs)\n",
    "    \n",
    "    plt.show()    \n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at another set of data. This is unemployment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import unemployment data from csv\n",
    "# CSV file was downloaded from www.bls.gov/lau\n",
    "\n",
    "# encoding is used since the text itself is formatted in a particular way\n",
    "u_data = pd.read_csv(\n",
    "    \"countyUnemploymentData.csv\", encoding = \"latin1\", parse_dates = True)\n",
    "\n",
    "# drop observations with missing fips codes\n",
    "# .index says show us the index\n",
    "index = u_data[\"fips_code\"].dropna(axis = 0).index\n",
    "\n",
    "# .loc says access U_data for all index\n",
    "u_data = u_data.loc[index]\n",
    "\n",
    "u_data[\"fips_code\"] = u_data[\"fips_code\"].astype(int)\n",
    "u_data.set_index([\"fips_code\", \"date\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can view all the geo-coordinates of county 1069\n",
    "u_data.loc[1069, \"geometry\"]\n",
    "\n",
    "\n",
    "#u_data.loc[1069, \"geometry\"].loc[\"Feb-20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COVID19Map.py\n",
    "#. . . \n",
    "# this function is a simplified version of the Create_Covid_geo_dataframe function we created above\n",
    "def create_merged_geo_dataframe(data, map_data, dates):\n",
    "    data_frame_initialized = False\n",
    "    # identify the counties before the for loop will make the code to run faster\n",
    "    counties = sorted(\n",
    "        list(\n",
    "            set(data.index.get_level_values(\"fips_code\"))))\n",
    "    for date in dates:\n",
    "       \n",
    "        # select county observations from each date in dates\n",
    "        # select only the subset of counties in the map that\n",
    "        # are also present in the covid_data\n",
    "        agg_df = map_data.loc[counties]\n",
    "        agg_df[\"date\"] = date\n",
    "        if data_frame_initialized == False:\n",
    "            \n",
    "            #Create new dataframe\n",
    "            matching_gpd = geopandas.GeoDataFrame(agg_df, crs = map_data.crs)\n",
    "            data_frame_initialized = True        \n",
    "        \n",
    "        else:\n",
    "            # or stack thenew  data frame and the dataframe that was initialized at\n",
    "            # i = 0\n",
    "            matching_gpd = matching_gpd.append(agg_df, ignore_index = False)\n",
    "            \n",
    "    # reset index, then set to [fips, date]\n",
    "    matching_gpd.reset_index(inplace=True)\n",
    "    matching_gpd.set_index([\"fips_code\", \"date\"], inplace = True)\n",
    "    \n",
    "    for key, val in data.items():\n",
    "        matching_gpd[key] = val\n",
    "    return matching_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the dates\n",
    "dates = [\"Aug-19\",\n",
    "         \"Sep-19\",\n",
    "         \"Oct-19\",\n",
    "         \"Nov-19\",\n",
    "         \"Dec-19\",\n",
    "         \"Jan-20\",\n",
    "         \"Feb-20\",\n",
    "         \"Mar-20\",\n",
    "         \"Apr-20\",\n",
    "         \"May-20\",\n",
    "         \"Jun-20\",\n",
    "         \"Jul-20\",\n",
    "         \"Aug-20\",\n",
    "         \"Sep-20\"]\n",
    "\n",
    "# another way of selecting the dates.\n",
    "#dates = list(set(u_data.index.get_level_values(\"date\")))\n",
    "\n",
    "u_data = create_merged_geo_dataframe(u_data, map_data, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose map bounds\n",
    "\n",
    "#if \"u_map_bounded\" not in locals():\n",
    "minx = -127\n",
    "miny = 23\n",
    "maxx = -66\n",
    "maxy = 50\n",
    "u_data = select_data_within_bounds(u_data, minx, miny, maxx, maxy)\n",
    "u_map_bounded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = sorted(list(set(u_data.index.get_level_values(\"fips_code\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"Unemployment Rate\"\n",
    "\n",
    "# csv saved data as string, transform to float\n",
    "u_data[key] = u_data[key].astype(float)\n",
    "\n",
    "# create new pdf\n",
    "pp = PdfPages(\"County Unemployment Rate.pdf\")\n",
    "for date in dates:\n",
    "    fig, ax = plt.subplots(figsize=(19,9),\n",
    "                          subplot_kw = {\"aspect\":\"equal\"})\n",
    "    plt.rcParams.update({\"font.size\": 30})\n",
    "\n",
    "    plt.xticks(fontsize = 25)\n",
    "    plt.yticks(fontsize = 25)\n",
    "\n",
    "    # set range of color bar. Min is 0, max is 20 and divide it to 4 so color bar will be 5,10,15,20\n",
    "    vmin = 0\n",
    "    #vmax = u_data[key].fillna(0).max()\n",
    "    vmax = 20\n",
    "    cmap = cm.get_cmap(\"Reds\", 4)\n",
    "    norm = cm.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    ### add colorbar\n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    # empty array for the data range\n",
    "    sm.A = []\n",
    "    # prepare space for colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    size = \"5%\"\n",
    "    # we'll place the colorbar on the right of the map\n",
    "    cax = divider.append_axes(\"right\", size = size, pad = .1)\n",
    "    # add colorbar to figure\n",
    "    cbar = fig.colorbar(sm, cax=cax, cmap= cmap)\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    vals = list(cbar.ax.get_yticks())\n",
    "    vals.append(vmax)\n",
    "    cbar.ax.set_yticklabels(vals)#[int(x) for x in vals])\n",
    "    cbar.ax.set_ylabel(key, fontsize=20)\n",
    "\n",
    "    # select data only from date\n",
    "    df = u_data[u_data.index.get_level_values(\"date\") == date]#.dropna(axis=0)\n",
    "    df.plot(ax=ax, cax=ax, column = key,\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            cmap=cmap, legend=False, \n",
    "            linewidth = .5, edgecolor=\"lightgrey\",norm=norm)\n",
    "    ax.set_title(date.replace(\"-\", \" 20\"))\n",
    "    plt.show()\n",
    "    pp.savefig(fig, bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "# close your pdf\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Unemployment Feb-20 == 1\n",
    "key = \"Unemployment Rate\"\n",
    "\n",
    "# df.copy() makes a copy of the dataframe\n",
    "# n_u_data represents normalized data and we'll normalize the data to 1\n",
    "n_u_data = u_data.copy()\n",
    "\n",
    "# go through data for every county for the key and divide the value by the value of the key\n",
    "# at the date that you would to normalize to 1\n",
    "# what this does is, it makes Febrary the standard to which data for all other months will be compared to\n",
    "# so, we are dividing all the employer data for each month by the unemployment data in february.\n",
    "# this will help us see the changes that occured in each month relative to February. \n",
    "for county in counties:\n",
    "    n_u_data[key][county] = n_u_data.loc[county, key].div(n_u_data.loc[county, \"Feb-20\"][key])\n",
    "    \n",
    "# alternatively, take the difference between the observed rate and the Feb rate:\n",
    "# for county in countries:\n",
    "#     n_u_data[key][county] = n_u_data.loc[county, key].sub(n_u_data.loc[county, \"Feb-20\"][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PdfPages(\"Normalized County Unemployment Rate.pdf\")\n",
    "\n",
    "for date in dates:\n",
    "    # accomplishes same outcome as date.replace(\"-\", \" 20\")\n",
    "    title_date = date[:-3]+\" 20\" + date[-2:]\n",
    "    fig, ax = plt.subplots(figsize=(19,9),\n",
    "                          subplot_kw = {\"aspect\":\"equal\"})\n",
    "    plt.rcParams.update({\"font.size\": 30})\n",
    "\n",
    "    plt.xticks(fontsize = 25)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    \n",
    "    # set range color bar values\n",
    "    vmin = 0 # n_u_data[\"Unemployment Rate\"].min()\n",
    "    vmax = 10\n",
    "    cmap = cm.get_cmap(\"Reds\", 10)\n",
    "#    cmap = cm.get_cmap(\"coolwarm\", 12)\n",
    "    norm = cm.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    ### add colorbar\n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    # empty array for the data range\n",
    "    sm.A = []\n",
    "    # prepare space for colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    size = \"5%\"\n",
    "    cax = divider.append_axes(\"right\", size = size, pad = .1)\n",
    "    # add colorbar to figure\n",
    "    cbar = fig.colorbar(sm, cax=cax, cmap= cmap)\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    vals = list(cbar.ax.get_yticks())\n",
    "    vals.append(vmax)\n",
    "    cbar.ax.set_yticklabels(vals)#[int(x) for x in vals])\n",
    "    cbar.ax.set_ylabel(\"Normalized \"+key + \"\\n(Feb 2020 = 1)\", fontsize=20)\n",
    "    \n",
    "    df = n_u_data[n_u_data.index.get_level_values(\"date\") == date]#.dropna(axis=0)\n",
    "    df.plot(ax=ax, cax=ax, column = key,\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            cmap=cmap, legend=False, \n",
    "            linewidth = .5, edgecolor=\"lightgrey\",norm=norm)\n",
    "    ax.set_title(title_date)\n",
    "    plt.show()\n",
    "    pp.savefig(fig, bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "\n",
    " # close pdf   \n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that unemployment rates tended to increase in regions hardest hit by COVID-19 early on. While the correlation is not perfect, the jump in unemployment in the northeast, including New Jersey, and in Michigan seem to follow this trend. Still, other factors matter. For example, the fall in oil prices seems to have impacted western Texas and western North Dakota. And although spread was limited on the West Coast, especially in California, a relatively strict lockdown there seems to be associated with higher levels of unemployment.\n",
    "\n",
    "The interested student would benefit from running a panel regression [see Chapter 8](https://github.com/jlcatonjr/Learn-Python-for-Stats-and-Econ/blob/master/Textbook/Chapter%208%20-%20Advanced%20Data%20Analysis.ipynb) that includes factors like the stringency of lockdown and the severity of spread of COVID-19, along with other variables relevant to the level of unemployment, to estimate the effects of these elements on the overall level of unemployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
